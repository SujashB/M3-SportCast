Great — I’ll put together a detailed research report on designing a unified architecture that integrates agentic workflows into Vision-Language Models for multilingual AI sports understanding. This will focus on real-time and offline fencing video analysis and commentary generation, leveraging VideoMAE, SAM/Samurai for optical focus on players, and a knowledge graph for contextual reasoning. I’ll also include how to integrate multilingual support (like Aya models), and deeply explore the implementation-level details of such a system.

I’ll get started and let you know once the report is ready.

# Unified Architecture for Agentic Vision-Language Sports Commentary (Fencing Case Study)

## Core Architecture Overview

To achieve multilingual AI sports understanding and human-like commentary for fencing, we propose a **unified architecture** composed of four key layers: **Perception (Vision)**, **Knowledge Reasoning**, **Language Generation**, and an **Agentic Orchestration** layer that binds them. The system integrates state-of-the-art models for video and language processing, domain-specific knowledge graphs, and real-time processing components. **Figure 1** outlines the main components and their roles:

| **Component**                 | **Role in System**                                            | **Technologies / Models**                                |
|-------------------------------|---------------------------------------------------------------|----------------------------------------------------------|
| **Perception Layer**          | Analyze video input; detect and classify fencing actions      | *VideoMAE* (spatiotemporal encoder) ([[2205.09113] Masked Autoencoders As Spatiotemporal Learners](https://arxiv.org/abs/2205.09113#:~:text=randomly%20mask%20out%20spacetime%20patches,datasets%20using%20vanilla%20Vision%20Transformers)) ([VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training | OpenReview](https://openreview.net/forum?id=AhccnBXSne#:~:text=more%20effective%20video%20representations%20during,training%20and)); *SAM/SAMURAI* (segmentation & tracking) ([[2304.02643] Segment Anything](https://arxiv.org/abs/2304.02643#:~:text=,1B%29%20of%201B%20masks%20and)) ([[2411.11922] SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory](https://arxiv.org/abs/2411.11922#:~:text=introduces%20SAMURAI%2C%20an%20enhanced%20adaptation,AO)) |
| **Knowledge Graph**           | Domain ontology for fencing rules, techniques, and context    | *Neo4j* graph DB (fencing ontology); queried via *Cypher* or *GraphQL* ([Knowledge Graph: The Best Friend of Generative AI | by Bojan Ciric | The Future of Data | Medium](https://medium.com/the-future-of-data/knowledge-graph-the-best-friend-of-generative-ai-f9e6e0f42df0#:~:text=Enhancing%20Generative%20AI%20with%20RAG,of%20reliable%20and%20rich%20knowledge)) |
| **Language Generation Layer** | Produce play-by-play commentary and analysis in multiple languages | *Multilingual VLM* (e.g. *Aya* 35B LLM) ([[2402.07827] Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model](https://arxiv.org/abs/2402.07827#:~:text=,evaluation%2C%20and%20simulated%20win%20rates)); translation & TTS for audio output ([Integrated AI System for Real-Time Sports Broadcasting: Player Behavior, Game Event Recognition, and Generative AI Commentary in Basketball Games](https://www.mdpi.com/2076-3417/15/3/1543/pdf?version=1738592773#:~:text=actions%20such%20as%20running%2C%20dribbling%2C,This%20system%20offers%20a)) ([Integrated AI System for Real-Time Sports Broadcasting: Player Behavior, Game Event Recognition, and Generative AI Commentary in Basketball Games](https://www.mdpi.com/2076-3417/15/3/1543/pdf?version=1738592773#:~:text=game%2C%20and%20Fotmob%20allows%20users,Over%20the%20last%20few)) |
| **Agentic Orchestration**     | Coordinate Perception→Reasoning→Generation workflow (real-time) | Agent framework (e.g. custom Python orchestration or toolkit like NVIDIA *AgentIQ* ([NVIDIA Unveils AI-Q Blueprint to Connect AI Agents for the Future of Work | NVIDIA Blog](https://blogs.nvidia.com/blog/ai-agents-blueprint/#:~:text=AI,data%20analytics%20for%20enhanced%20intelligence)) or *LangChain*) |

**VideoMAE** serves as the visual backbone for the Perception layer. VideoMAE is a Masked Autoencoder pre-trained on video data, enabling efficient *spatiotemporal representation learning* ([[2205.09113] Masked Autoencoders As Spatiotemporal Learners](https://arxiv.org/abs/2205.09113#:~:text=randomly%20mask%20out%20spacetime%20patches,datasets%20using%20vanilla%20Vision%20Transformers)). It learns holistic video features by reconstructing masked video patches with an extremely high masking ratio (90–95%), yielding strong action representations even with limited data ([VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training | OpenReview](https://openreview.net/forum?id=AhccnBXSne#:~:text=more%20effective%20video%20representations%20during,training%20and)). These rich features are crucial for understanding complex fencing motions (lunges, parries, ripostes) over time.

For focusing on relevant visual regions, we incorporate the **Segment Anything Model (SAM)** by Meta. SAM is a promptable segmentation model trained on **1 billion masks** that can zero-shot segment objects in new images with impressive accuracy ([[2304.02643] Segment Anything](https://arxiv.org/abs/2304.02643#:~:text=,1B%29%20of%201B%20masks%20and)). In our system, SAM is used to isolate the two fencers (and possibly their weapons) from the background, given minimal prompts (e.g. an initial bounding box or point on each fencer). This *optical focus* ensures downstream analysis (e.g. action recognition) is concentrated on the players. For continuous video, we leverage **SAMURAI**, an enhancement of SAM designed for *real-time object tracking* in video ([[2411.11922] SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory](https://arxiv.org/abs/2411.11922#:~:text=introduces%20SAMURAI%2C%20an%20enhanced%20adaptation,AO)). SAMURAI introduces motion-aware memory to track segmented objects across frames without retraining, achieving robust tracking of fast-moving, occluding targets (as seen in sports) in a zero-shot manner ([[2411.11922] SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory](https://arxiv.org/abs/2411.11922#:~:text=introduces%20SAMURAI%2C%20an%20enhanced%20adaptation,AO)). Using SAM/SAMURAI, the system can maintain a high-quality segmentation mask on each fencer throughout the bout, even during rapid exchanges, enabling consistent identification of which athlete is performing an action.

The **Multilingual Vision-Language Model (VLM)** is at the core of the Language Generation layer. We choose an open-access model such as **Aya** – a *massively multilingual* generative LLM that follows instructions in **101 languages** (over 50% being low-resource) ([[2402.07827] Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model](https://arxiv.org/abs/2402.07827#:~:text=,evaluation%2C%20and%20simulated%20win%20rates)). Aya is fine-tuned from a 13B backbone (mT5) and *outperforms prior multilingual models (mT0, BLOOMZ)* on most tasks, while covering twice as many languages ([[2402.07827] Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model](https://arxiv.org/abs/2402.07827#:~:text=breakthroughs%20beyond%20first,evaluation%2C%20and%20simulated%20win%20rates)). This broad language capability is essential for global sports commentary. The Aya model (35B variant for V+L tasks) can be integrated to understand prompts about visual events and generate commentary in the requested language. If needed, the architecture can support *multiple VLMs or translation tools* – for example, generating an English commentary and then translating via a model like NLLB (No Language Left Behind) for languages outside the LLM’s 101-language set. However, a single multilingual model like Aya streamlines the pipeline by directly producing commentary in the target language upon command.

A **Knowledge Graph (KG)** provides deep domain context and factual grounding, enabling *analytics-driven commentary*. We design a **Neo4j-based fencing ontology** that encodes the rules, techniques, historical data, and relationships relevant to fencing. For example, the KG contains nodes for **Techniques** (thrust, parry, fleche, etc.) linked to their descriptions and tactical implications; nodes for **Players** with attributes like nationality, past records, preferred techniques; nodes for **Matches** and **Touch events** with relationships indicating which technique scored, on which target, at what time. This structured representation of knowledge allows complex queries – e.g., “find how often fencer A’s fleche attack succeeds against fencer B” or “retrieve the definition of a ‘parry riposte’ and famous instances in past finals.” The KG can be queried using **Cypher** (Neo4j’s query language) or exposed via a **GraphQL** API for easy integration. Integrating a KG with the generative model via *Retrieval-Augmented Generation (RAG)* greatly boosts the accuracy and richness of AI outputs: the commentary can be grounded in **reliable, up-to-date facts** from the KG, reducing hallucinations and adding insightful context ([Knowledge Graph: The Best Friend of Generative AI | by Bojan Ciric | The Future of Data | Medium](https://medium.com/the-future-of-data/knowledge-graph-the-best-friend-of-generative-ai-f9e6e0f42df0#:~:text=Enhancing%20Generative%20AI%20with%20RAG,of%20reliable%20and%20rich%20knowledge)). In short, the KG serves as the memory of the system’s “brain,” ensuring the commentary isn’t just visually descriptive but also *analytically informed* (e.g. explaining *why* a move is significant or contextualizing it historically).

All these components are unified under an **Agentic Orchestration Layer**. This layer coordinates the workflow of specialized modules (which we can think of as *agents*) in a seamless pipeline. It ensures that in real-time operation, the modules communicate and hand off data fluidly – e.g., triggering the knowledge graph lookup after an action is recognized, then invoking the language model with both the perceived event and retrieved context. The orchestrator can be implemented as a rule-based controller (for deterministic pipelines) or as an agentic framework where an AI agent dynamically calls tools. For example, frameworks like NVIDIA’s **AgentIQ** enable connecting teams of AI agents and tools with full traceability and optimized performance ([NVIDIA Unveils AI-Q Blueprint to Connect AI Agents for the Future of Work | NVIDIA Blog](https://blogs.nvidia.com/blog/ai-agents-blueprint/#:~:text=The%20blueprint%20is%20powered%20by,in)) ([NVIDIA Unveils AI-Q Blueprint to Connect AI Agents for the Future of Work | NVIDIA Blog](https://blogs.nvidia.com/blog/ai-agents-blueprint/#:~:text=AI,data%20analytics%20for%20enhanced%20intelligence)). In our case, the orchestrator can be a lightweight Python service that listens to events from the perception module (e.g., “Touch detected”), then invokes a *Reasoning Agent* to query Neo4j, and finally triggers a *Generation Agent* (the VLM) to produce commentary. By encapsulating each stage as an agent, we allow autonomous operation and potential parallelism, all within a *unified architecture*. This modular yet integrated design makes it easier to maintain the system (each component can be improved independently) while the orchestrator ensures they work in concert in real-time.

## Agentic Workflow Design

The system operates through an **agentic workflow** that mirrors a human commentator’s process of perceiving, understanding, and narrating. We define distinct agents for **Perception**, **Reasoning**, and **Generation**, coordinated in a pipeline. Below is the chained multi-agent workflow for a typical fencing commentary cycle:

1. **Perception Agent (Vision)** – This agent continuously *watches the fencing video feed and detects events*. It uses the VideoMAE encoder and SAM/SAMURAI to process each frame or short sequence. First, it applies SAM to focus on the two fencers and possibly the scoring apparatus (e.g. tip lights or electronic score display). Within those regions, the agent analyzes motion and posture features via VideoMAE to recognize *what action is occurring*. For example, it might classify an action as “Fencer A initiates a lunge” or detect a blade contact indicating a touch. This may involve a fine-tuned classifier on VideoMAE features to categorize fencing techniques or an object detector to spot the flash of a scoring light. The Perception agent consolidates this into a structured **event descriptor** – e.g., *{actor: A, action: “fleche attack”, result: “touch scored on B’s torso”}*. It operates autonomously on the video stream, posting events whenever something notable happens (point scored, referee calls halt, etc.). Multiple perception sub-agents can be employed: one for *action recognition*, another for *score detection*, and another for *player tracking*, all working in tandem. Multi-agent coordination here means, for instance, a **Segmentation sub-agent** provides masks to an **Action Recognition sub-agent** which then yields an event classification.

2. **Reasoning Agent (Knowledge Graph Querying)** – Once an event is detected, the Reasoning agent awakens to *provide context and deeper insights*. It takes the event descriptor from perception and queries the **fencing knowledge graph** for relevant information. For example, if the event is “fleche by A scored on B”, the agent might query: *What is a fleche?*; *retrieve A’s historical success rate with fleche*; *find notable instances of fleche in fencing history or B’s weakness against fleche*. These queries are executed via Cypher on the Neo4j ontology or via a GraphQL interface. The KG reasoning can also attach real-time context – e.g., *current score*, *time remaining*, or *penalties on record*. The result is a set of **contextual facts** or insights. For our example, the KG might return data like: *“Fleche: a running attack where the fencer leaps at the opponent (defined in ontology)”*; *“Fencer A used fleche to win the 2023 Championship finals”*; *“B’s reaction to fleche is often slow (derived from past match data)”*. The Reasoning agent distills these results into a **knowledge context package** (essentially a few key facts or sentences) that will enrich the commentary. This agent acts autonomously, governed by logical rules or queries defined in advance (it could also use an LLM to decide *which* queries to run, but an explicit mapping of event types to query templates is more deterministic and faster). By chaining the Reasoning agent after perception, we ensure the commentary has *depth*: beyond describing the action, it can explain significance or strategies, much like a human expert who recalls relevant facts on the fly.

3. **Generation Agent (Commentator LLM)** – This agent is responsible for *turning the structured information into natural, engaging commentary* in the desired language. It receives input from both the Perception agent (the raw event descriptor) and the Reasoning agent (the contextual knowledge). For example, it might be prompted with a summary like: *“Event: Fencer A scored with a fleche attack on Fencer B’s torso, earning a point. Context: A fleche is a swift running attack. [Fencer A] famously used this move in the 2023 finals. [Fencer B] has struggled to defend against fleche attacks historically.”* The Generation agent then uses the multilingual VLM (Aya or similar) to compose a fluent commentary sentence or two. Because Aya is instruction-tuned across many languages, we can prepend a directive like *“Language: French”* or provide the user’s language choice, and the agent will output in that language. For example, in English it might generate: *“Wow! [Fencer A] lands a spectacular **fleche** to the torso of [Fencer B]! This signature move – a running attack – pays off again; recall that [Fencer A] used it to win the championship in 2023. [Fencer B] has a known weakness against such blitz attacks.”* The agent aims for a **human-like style**: using excitement, varying sentence structure, and even brief dramatic pauses (which can be inserted later in TTS). We can maintain a library of prompt templates to enforce style, e.g., *“Respond as an enthusiastic sports commentator, in present tense, with concise sentences.”* Because the Generation agent has the factual context from the KG, it grounds its narration in truth, reducing hallucination. Multi-agent coordination here means the Generation agent might call back to the Reasoning agent if something is unclear (though in our design, one pass is usually enough since reasoning pre-fetches relevant info). The output is a text commentary snippet ready for delivery.

4. **Orchestrator & Multi-Agent Coordination** – Overseeing these three specialized agents is the Orchestrator (which can be thought of as a high-level agent or simply the control logic). It ensures the agents work in sequence and timing appropriate for *real-time performance*. For instance, in a live scenario, the Orchestrator might maintain a small buffer of the video stream (a few seconds) to give the Perception agent time to recognize an action and still synchronize commentary output. As soon as the Perception agent flags an event, the Orchestrator launches the Reasoning and Generation agents (these can work in parallel: the KG query and LLM generation can overlap if the LLM is prompted with known info while KG provides additional facts to append). The Orchestrator handles *threading* and data passing: e.g., packaging the perception result and sending it as parameters in a Cypher query to the KG, then combining KG results with a prompt template for the LLM. This design can be implemented using an async event loop or a message bus where each agent subscribes to certain event types. **Multi-agent coordination** is thus achieved by a publish/subscribe pattern or an internal API: the Perception agent publishes “action_detected” events, the Reasoning agent listens for those and responds with “context_ready”, and the Generation agent awaits both to produce “commentary_ready”. An important aspect of the orchestration is error handling and fallback – e.g., if the KG has no info on a very novel move, the Reasoning agent could at least supply a rule or nothing, and the Generation agent should still produce a meaningful commentary (perhaps focusing on the basic description). The orchestrator agent ensures the *chain-of-thought* remains coherent across modalities. This approach aligns with emerging multimodal agent frameworks, where agents leverage different modalities (vision, text, structured data) to achieve a task autonomously and collaboratively ([Mastering Function and Tool Calling for Advanced Reasoning and Multimodal Frameworks | by Emmanuel Mark Ndaliro | Medium](https://medium.com/@kram254/mastering-function-and-tool-calling-for-advanced-reasoning-and-multimodal-frameworks-fcfe6ffb6aaf#:~:text=Multimodal%20Agentic%20Frameworks)). By designing the workflow as a team of cooperating agents, the system gains **flexibility** (easily add a new agent, like a *Referee call agent* to listen for referee signals) and **robustness** (if one agent fails, the system can attempt recovery or defaults).

In summary, the agentic workflow design enables **autonomous operation across perception, reasoning, and generation**. Each agent specializes in a part of the pipeline but shares information to produce a final outcome greater than the sum of parts – much like human experts in a live broadcast (one person might be spotting the action, another recalling stats, and the lead commentator narrating). The multi-agent coordination is key for chaining tasks such as *segment → detect technique → query knowledge → generate commentary* smoothly. Thanks to this design, the system can operate in **real-time**, with the priority agent (Perception) running continuously and the downstream agents activating on-demand with minimal latency.

## Technological Implementation

Implementing this unified architecture requires integrating several advanced tools and frameworks in a performance-optimized manner. We break down the implementation details by component:

### Video Perception Implementation (VideoMAE + SAM)

**Video Stream Ingestion** – We assume a video input (live feed or recorded footage) at a reasonable resolution (e.g. 1080p at 30 FPS). We use OpenCV (cv2) in Python or an FFmpeg-based pipeline to capture frames in real-time. Frames are forwarded to the perception module running in a separate thread or process to maximize concurrency.

**Spatiotemporal Encoding with VideoMAE** – We load a pre-trained **VideoMAE** model (e.g., the NeurIPS 2022 VideoMAE model by Zhan et al. ([VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training | OpenReview](https://openreview.net/forum?id=AhccnBXSne#:~:text=than%20that%20of%20images,Code%20is))) using PyTorch. The model is either fine-tuned on a fencing-specific action dataset or used as a feature extractor. Implementation-wise, we wrap VideoMAE in a PyTorch `nn.Module` that accepts a window of frames (e.g., 16 or 32 frames ~0.5–1s of video) and outputs a latent representation (from the encoder) or an action class prediction if fine-tuned for classification. The **efficiency** of VideoMAE is critical: with a high masking ratio during pretraining, it’s very *data-efficient* and we can fine-tune it with only a few hundred labeled fencing sequences to recognize key actions ([VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training | OpenReview](https://openreview.net/forum?id=AhccnBXSne#:~:text=more%20effective%20video%20representations%20during,training%20and)). Moreover, during inference the model is lightweight (depending on the backbone size – a base ViT vs large ViT). We can leverage PyTorch’s CUDA acceleration and possibly half-precision (FP16) to speed up inference. If needed for real-time, *temporal stride* can be used: e.g., process every Nth frame or use a rolling window that advances, say, 5 frames at a time instead of every single frame.

**Action Recognition** – On top of VideoMAE’s features, we implement a small classifier to map visual patterns to fencing actions. This could be a linear layer (if fine-tuning end-to-end) or a separate model like an LSTM/Transformer that processes the sequence of VideoMAE embeddings over time. Another approach is to use an existing action recognition model like **R(2+1)D** (ResNet (2+1)D) as in prior work ([Integrated AI System for Real-Time Sports Broadcasting: Player Behavior, Game Event Recognition, and Generative AI Commentary in Basketball Games](https://www.mdpi.com/2076-3417/15/3/1543#:~:text=and%20player%20details.%20YOLO%20model,The%20system%20demonstrates%20high)), or a **SlowFast** CNN, and compare it with VideoMAE. In our pipeline, VideoMAE is preferred for its modern transformer-based representation, but one could ensemble it with a CNN-based detector for increased accuracy (at cost of speed). We maintain a label set of relevant actions: e.g., *attack (lunge, fleche)*, *defensive action (parry, dodge)*, *point scored*, *off-target hit*, *halt*, etc. The output includes *who* performed the action – for which we rely on tracking (see below).

**Segmentation and Tracking with SAM/SAMURAI** – We integrate Meta’s Segment Anything Model via its open-source APIs (Facebookresearch’s `segment-anything` library). At system initialization, we prime SAM with the targets: for example, we might detect persons in the first frame using a quick person detector (YOLOv8) and then place point prompts in the center of each detected person for SAM to generate precise masks. Once we have the fencers’ masks, we feed them along with subsequent frames to the **SAMURAI** tracker (the SAMURAI code from the official GitHub ([SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual ...](https://github.com/yangchris11/samurai#:~:text=,Aware%20Memory)) can be used). SAMURAI will update the masks for each new frame, keeping track of which mask corresponds to which fencer using an ID. This gives us per-frame localized regions for each athlete. The *optical focus* is then achieved by cropping or masking out everything outside the fencers before feeding frames to VideoMAE, or by only computing VideoMAE features inside the mask area (to save computation). Because SAM is highly optimized (using a ViT-H backbone that runs on GPU and a lightweight mask decoder), and SAMURAI’s tracking is designed for real-time, we expect this segmentation+tracking to run at perhaps ~10–15 FPS on a single high-end GPU. This is a potential bottleneck; if needed, optimizations include running SAM at a lower resolution or every few frames (interpolating masks in between), or using a simpler tracker (like MOSSE or KCF on the masks). In practice, SAMURAI has demonstrated real-time performance with zero-shot tracking by leveraging temporal cues ([[2411.11922] SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory](https://arxiv.org/abs/2411.11922#:~:text=motion,its%20robustness%20in%20complex%20tracking)), so on a powerful GPU (e.g., NVIDIA A100), maintaining ~30 FPS tracking for two objects is achievable.

**Real-Time Considerations** – The perception pipeline is the most time-sensitive. We adopt a *producer-consumer* model: frames are captured and placed into a queue; the perception agent pulls frames, possibly batches them (for VideoMAE, batch=1 sequence), and processes. If the processing falls behind real-time, the queue can drop old frames (ensuring we focus on the latest moment). This trade-off means we might skip analyzing some frames, but it prevents buildup of latency. In parallel, we keep a lightweight process to monitor for scoring events – for instance, if the fencing scoring system’s LED is visible, a simple color/intensity threshold detector on the region of the lights could instantly signal a touch (this could even be another perception sub-agent that directly triggers commentary for scoring, independent of VideoMAE). Such parallelization ensures that high-level events (score changes) are caught with minimal delay, while finer action recognition might come a few frames later. We also utilize **PyTorch CUDA streams** to overlap computation: while VideoMAE is processing on the GPU, the next frame capture can happen on CPU, and SAM’s CPU post-processing of masks can overlap with VideoMAE GPU work, etc.

### Knowledge Graph Integration (Neo4j + GraphQL)

We implement the **fencing ontology** in Neo4j, designing the schema (using labels like `Player`, `Technique`, `Bout`, `Event`). For example, a `Technique` node “fleche” might have relations like `:IS_COUNTER_TO` another technique, or `:CATEGORY -> Attack`. A `Player` node has properties (country, age, etc.) and relations `:USES -> Technique` (with frequency), or `:HAS_RECORD -> Match`. We load historical data: past match outcomes, players’ stats, etc., into the graph. We also encode **rules** – e.g., in sabre, `Technique:Cut` has property target_area = upper_body. This structured info can be manually curated and augmented over time.

To enable queries from the Reasoning agent, we have two options: use the Neo4j **Python driver** to run Cypher queries or set up Neo4j’s **GraphQL endpoint** (via Neo4j GraphQL Library) to allow semantic queries. We choose to implement a set of pre-defined Cypher query templates for known needs, as this is straightforward. For instance: 

- Template 1: Given `Technique X` and `Player Y`, retrieve Y’s success rate with X (this would match patterns like `(y:Player)-[r:USED_IN]->(e:Event {technique:X})` and aggregate outcomes).
- Template 2: Given two players A and B, find last N encounters and outcomes.
- Template 3: Given `Technique X`, get its description and related tactics.

These queries are stored, and the Reasoning agent fills in parameters (like the names) when an event triggers a query. The Neo4j database runs in the background (it can be on the same machine or a separate server accessible via network). Neo4j is optimized for such graph traversals, so queries should return in milliseconds for our scale (the data is not massive – perhaps a few thousand nodes and relationships). If needed, we enable **full-text indexes** on certain properties (like technique name, player name) to speed up text queries.

One powerful aspect is using the KG for *real-time context updates*. As the match proceeds, we log events into the KG as well (or a parallel in-memory graph). For example, each touch is added as an `Event` node linked to players and technique. This means by mid-match, the KG can answer questions like “how many touches has A scored with attacks vs counter-attacks”. The Reasoning agent can utilize this to comment on emerging patterns (“3 of A’s points came from counter-parries, showing her defensive prowess”).

Security and reliability are considered – we ensure that queries are controlled (the LLM isn’t directly generating Cypher to avoid any unpredictable queries; instead the agent code selects appropriate queries). The **GraphQL** interface is an alternative if we wanted the LLM to compose queries in a safer graph query language, but likely not needed with our templating approach.

### Commentary Generation (Multilingual LLM + Tools)

**LLM Integration (Aya model)** – We host the Aya 35B multilingual model, which may require significant GPU memory (35B parameters typically need at least ~4×24GB GPUs if loaded in full precision; using 8-bit quantization, it might fit on a single 48GB GPU). We can use Hugging Face Transformers library to load the model with `model = AutoModelForCausalLM.from_pretrained("CohereForAI/aya-35b", device_map="auto", load_in_8bit=True)`. This will spread the model across available GPUs and use efficient 8-bit inference. The generation agent constructs a prompt with the data from perception and reasoning. We maintain prompts for different languages; for example:

```
<<System>>: You are an expert fencing commentator AI. Describe the fencing action and its context in an engaging, concise manner.
<<User>>: [Language=Spanish] 
Event: Fencer A scores with a fleche on Fencer B.
Context: Fencer A often uses fleche (a running attack) successfully. Fencer B struggled against fleche in past matches.
<<Assistant>>:
```

The model then generates the commentary in Spanish in this case. Because Aya is instruction-tuned, it respects the language tag and the role (we can fine-tune or few-shot it to be even more commentator-like). If Aya or an open model is not available for some languages, we can integrate a translation tool. For instance, after generating English text, we could call an external translation API or an open-source model like MarianMT or M2M-100 to translate to the target language. However, using a single multilingual model avoids inconsistencies and allows direct styling in that language (e.g., sports idioms specific to the language).

**Human-Like Commentary Styling** – We leverage the LLM’s training (which likely includes some informal or conversational data) to create a lively tone. We also can inject *commentator phrases* via the prompt or as a small fine-tune. For example, providing a few examples in the prompt like: *“<<Example>>: Wow, an amazing touch by [Player]! [He/She] executed a perfect [technique]…”* can guide the model. Implementation detail: we use **temperature** ~0.7 for some creativity (so it’s not too flat), and limit the length to maybe 2–3 sentences per event. We also post-process the output to ensure it fits the timing (if it’s too long, we might truncate or summarize further). The Generation agent runs on the GPU as well; if the same GPU is used for video, we might dedicate one GPU for the LLM and one for VideoMAE+SAM to avoid contention (this is part of **pipeline design for latency**, possibly having multi-GPU).

**Multilingual Support and Switching** – The system can be configured to a default language or multiple output languages. For real-time multi-language commentary, one approach is to run two Generation agents in parallel, each with a different language prompt (one English, one French, for example). This doubles the load, but since generation is relatively fast for short text (e.g., generating ~50 tokens might take <0.5s on a GPU), it’s feasible. Alternatively, generate in one language and translate to others concurrently using lightweight translators. The architecture supports either. We ensure the KG facts used are translated or mapped to each language (the KG might store multilingual labels for techniques, e.g., “fleche” is a French word commonly used in English commentary too, but a term like “attack au fer” we might need a localized description).

**Text-to-Speech (TTS) Integration** – To deliver audio commentary, we integrate a TTS engine. Modern TTS systems can produce natural speech that captures emotion. For flexibility, open-source solutions like **Coqui TTS** or **Mozilla TTS** can be used, or cloud APIs (Amazon Polly, Google Cloud TTS, etc.) for high-quality multi-language support. For a real-time system, low latency is key: many TTS APIs can synthesize one sentence (<15 words) in under 300ms. We choose voices that sound like sports commentators (some TTS services offer expressive styles). For example, the **NCSoft PAIGE** system mentioned in literature used TTS for *realistic, emotional broadcasting* ([Integrated AI System for Real-Time Sports Broadcasting: Player Behavior, Game Event Recognition, and Generative AI Commentary in Basketball Games](https://www.mdpi.com/2076-3417/15/3/1543/pdf?version=1738592773#:~:text=game%2C%20and%20Fotmob%20allows%20users,Over%20the%20last%20few)) – we aim for a similar effect. During deployment, the Generation agent’s output text is sent to the TTS module, which returns an audio clip (e.g. a WAV). We then play this audio (if live, send to speakers or broadcast; if offline, multiplex into the video). We make sure to pipeline this such that while one commentary audio is playing, the perception agent is already processing the next action – this overlap hides the TTS latency.

**Latency and Synchronization** – The end-to-end latency from event detection to spoken commentary is critical. We target that commentary for a touch is heard no more than ~1–2 seconds after the touch occurs. The breakdown might be: ~0.3s for perception to detect and confirm the event (could even be immediate if using sensor input), ~0.1s for KG query (negligible), ~0.5s for LLM generation, ~0.3s for TTS synthesis – roughly ~1.0–1.2s total, which is acceptable in live sports (often human commentators also react within a second or two). If needed, we can slightly *delay the video* to the viewer by 1–2s to ensure commentary is always in sync (this is a common broadcasting technique). For offline mode, synchronization is simpler: we align the timestamp of the event with the generated speech in post-production. The orchestrator can assign each commentary line a timestamp based on the video timeline it’s referring to.

We also incorporate **audio-visual synchronization** checks: The system knows the frame/time of the event, so it can tag the generated commentary with that timestamp. If the commentary audio is ready early, it can wait until the correct video time to play. If it’s late, the orchestrator might decide to still play it (slightly delayed commentary is better than none, and often viewers won’t notice a second delay). In addition, because we have the segmentation info, we could highlight the fencer or action in the video in sync with the commentary (for a richer experience) – e.g., flash an outline around the player as the commentary mentions them, but this is an optional broadcast feature.

**System Integration and APIs** – We implement the entire pipeline in Python using PyTorch for AI models and possibly FastAPI or gRPC to handle inter-process communication if we break components into services. For example, the perception module might run in one process (to fully utilize a GPU), the LLM in another (on another GPU), and they communicate via an internal API. The orchestrator could be part of the perception process or a separate coordinator process. We ensure that data passed (like event descriptors) are in a standard format (JSON or Protocol Buffers). Logging is also implemented: each event, KG query, and generated commentary is logged for debugging and for improving the system (this log can be used to refine the KG or tune the LLM prompts).

Crucially, we test the real-time loop extensively. We simulate a live feed and measure end-to-end times. Where latency is too high, we apply optimizations: e.g., reduce VideoMAE frame window size, limit the number of KG queries, use a smaller LLM model if needed (Aya 13B instead of 35B, if 35B is too slow). The architecture is designed to be **scalable** – for offline deep analysis we can afford the heaviest models (e.g., even running at 5 FPS but getting very detailed commentary), whereas for real-time, we scale down gracefully (fewer details, more template-based lines if necessary to meet timing). Techniques like model quantization, tensorRT optimization, and batching multiple tasks (if a multi-threaded CPU can batch two frames for VideoMAE when GPU is free) are employed as needed to hit performance targets.

In summary, the technological implementation brings together **PyTorch/CUDA** for model inference, **Neo4j/Cypher** for knowledge queries, and various Python libraries for video I/O and audio. The result is a pipeline that can ingest video, analyze it with *computer vision*, enrich it with *graph-based knowledge*, and produce *natural language commentary* in real-time. The design is modular: each part can be replaced with newer models (for instance, if a new video foundation model better than VideoMAE arises, or a new multilingual LLM) without altering the overall pipeline.

## Multilingual Commentary System

One of the standout features of this architecture is its ability to deliver commentary in **multiple languages**, providing a tailored experience for viewers worldwide. The commentary system consists of a *multilingual narration pipeline* and optional output modalities (text display, speech audio). Key aspects of the multilingual commentary design include:

**1. Pipeline from Perception/Reasoning to Narrative**: The transition from raw detections to a narrative sentence involves intermediate representation and templating. After the perception and reasoning steps, we have structured data about what happened and why it’s important. Before feeding this to the LLM, the system formats it into a **commentary prompt structure**. This structure can be thought of as a mini “context” in a few-shot learning sense. For example, we might create a JSON or YAML-like snippet:
``` 
event: {type: "Touch", scorer: "Alice", technique: "fleche", target: "arm"}
context: {technique_desc: "A fleche is a running attack", player_insight: "Alice won 3 points today with fleche"}
tone: "excited"
language: "English"
```
The LLM could be fine-tuned or prompted to map this structured input to a sentence. Another approach is to use **template-based generation with LLM refinement**: e.g., have a template “{scorer} scores with a {technique}! {context}.” then allow the LLM to rephrase or embellish it. However, given the power of modern LLMs, directly prompting usually yields more human-like variety.

**2. Multilingual Generation**: We instruct the LLM to output in the viewer’s language. Aya’s training on 101 languages means it has seen instructions and can likely produce coherent sports commentary in languages like English, Spanish, French, Chinese, Arabic, etc. If we need to handle code-switching or multiple languages simultaneously, we instantiate separate prompt+generate calls per language. The content (event and context) is the same, but we might adjust idioms. For instance, English commentary might say “scores with a fleche”, whereas French commentary might not translate “fleche” since it’s already French but might adjust phrasing (“quelle belle flèche !”). We rely on the LLM’s knowledge of language nuances for this, potentially augmented by examples in the prompt. If necessary, a small lookup table of key fencing terms translated in various languages can be provided (for terms that do have local equivalents).

**3. Quality and Style Control**: To ensure the commentary is *human-like*, we want it to be not only factually correct but also emotive and flowing. We leverage *few-shot examples* of real commentary. For instance, we might include in the system prompt: *“Example: [In English] Alice lunges and touches Bob’s shoulder! What a perfectly timed attack by Alice.”* and similarly for other languages if we have them. We also instruct the model to keep sentences reasonably short (for ease of TTS and listener comprehension). The model should avoid monotony – we can maintain a small list of interchangeable phrases (“What a touch!”, “Incredible move!”, “Beautiful timing!”) that the LLM can draw from. One way is to feed these as part of the context (like a style guide).

**4. Translation and Verification**: If the LLM does not support a particular language needed, the system falls back to translation. In that case, we generate in a high-resource language first (say English, which the KG info and model are strongest in), then use a translation model. We would then have a second LLM or a rule-based translator. There are excellent open models (e.g., M2M-100, or Helsinki-NLP’s OPUS MT models for many language pairs) that can be called via Hugging Face’s `transformers` pipeline. We would then have to trust the translation to preserve meaning. In practice, a verification step could be included – e.g., translate back to English and check it’s similar to the original. This might be overkill for live, but for offline mode quality assurance, it’s doable.

**5. Speech Synthesis and Rendering**: After text is generated in the target language, the system converts it to speech. We configure TTS with a **voice matching the language** (e.g., a Spanish voice for Spanish commentary). If multiple language commentaries are to be output simultaneously (e.g., different audio channels), we handle each language’s TTS separately. The TTS engine is invoked with the text string and returns an audio buffer. To synchronize, we align the audio playback with video. If this is a live system, we start playback immediately (or at an appropriate moment relative to the event). If offline, we place the audio clip on the timeline at the exact timestamp of the event (taking into account any delay the commentary style implies – e.g., sometimes commentators describe an action just after it finishes). Audio mixing considerations: if the original video has ambient sound (crowd noise, sword clinks), we mix the TTS voice over it, perhaps ducking the ambient volume slightly when the TTS plays, to mimic broadcast mixing.

**6. Additional Outputs (optional)**: The multilingual commentary text can also be shown as subtitles or on-screen graphics. In a broadcast scenario, we might display the commentary in text form for accessibility (especially if doing multiple languages – one can listen to one language and read another, theoretically). The system can output a subtitle file (e.g., WebVTT or SRT) with timestamps. Because we have exact event times, we can easily generate these subtitles with timing.

**Human-like Multilingual Nuances**: Each language has its own sports idioms. In Spanish one might say *“¡Punto para Alice!”* or in Italian *“Affondo riuscito!”* (successful lunge). Our knowledge graph could store some common phrases or the LLM might know them. If we wanted to be very fine-tuned, we could maintain separate style prompts per language. Implementation-wise, we could have a JSON config like:
```json
"language_styles": {
  "en": {"scorer_first": true, "exclamations": ["What a hit!", "Incredible!"]},
  "fr": {"scorer_first": false, "exclamations": ["Quelle touche!", "Incroyable!"]}
}
```
and so on, and feed this into the prompt. However, given the complexity, it might suffice to rely on the LLM and perhaps a human reviewer to correct any glaring style issues during development.

**Case Study Example**: Suppose in a fencing bout, we detect: *Player Kim (KOR) parries Player Lee’s attack and ripostes to score.* The reasoning agent finds that this is a classic move (*parry-riposte*), and maybe that Kim is known for strong defense. The generation agent in English might produce: *“Kim parries and lands a riposte on Lee – a textbook defensive move from the Korean fencer!”*. In French, it might say: *“Kim pare l’attaque de Lee et riposte pour marquer ! Un mouvement défensif parfait de la part du Coréen.”* The system ensures both are conveyed with the same meaning. The TTS voices speak these with appropriate intonation (excited when a point is scored). The multilingual system thus provides each audience with an equally rich commentary experience.

**Real-Time Language Switching**: If this were an interactive system (say a user can switch language on the fly), the system could dynamically change the target language for new commentary. Past commentary wouldn’t be re-spoken in the new language (though theoretically, we could maintain a transcript and translate it if needed). But for simplicity, language is chosen per session.

In summary, the multilingual commentary subsystem transforms the analytical outputs into *engaging narrative across languages*. It uses the LLM for natural language generation grounded in facts (via KG) and uses TTS for spoken output. This design ensures that whether the viewer speaks English, Spanish, Chinese, or Russian, they get a **live commentary experience** of the fencing match that feels authentic and informed, as if a knowledgeable human commentator is speaking their language.

## Optional Enhancements

Finally, we discuss some optional enhancements that can take the system further in terms of predictive analytics and immersive experience:

- **Temporal Sequence Modeling for Technique Prediction**: Beyond reacting to events, the system could *anticipate* them. By analyzing the temporal sequence of moves, the AI can predict what might happen next (much like expert commentators guessing an upcoming tactic). Implementing this could involve a specialized transformer that processes the timeline of recent actions. For example, an *Anticipative Video Transformer (AVT)* model ([[2106.02036] Anticipative Video Transformer](https://arxiv.org/abs/2106.02036#:~:text=,AVT%20obtains%20the%20best%20reported)) can be trained to predict the next action in a sequence by attending to past frames and actions. In fencing, this could let the AI detect that a fencer has been pushing the opponent to the end of the piste and predict a fleche or a desperate counter-attack. The commentary agent could then add speculative remarks (“It looks like Alice is setting up for a big attack here…”). This must be done carefully to not mislead (the model’s prediction should be fairly confident or phrased as speculation). Nonetheless, such predictive commentary can increase the *human-like feel*, as real commentators often forecast possibilities. The sequence model could be integrated as another agent that continuously monitors the state (perhaps reading from the KG’s log of events or directly from perception outputs) and triggers a commentary event “possible_next_move” with some probability. Even without a heavy model, simpler sequence rules can be encoded (e.g., if a fencer scored twice with the same move, predict the opponent will anticipate it next time). This adds a layer of strategic commentary.

- **Audio-Visual Synchronization and Enhanced Sportscast**: To create a *complete sportscast experience*, synchronization between audio commentary, on-screen visuals, and possibly other audio (crowd noise, sound effects) is crucial. One enhancement is to use the segmentation data to generate live graphics – for instance, a ghost trail showing the path of a successful attack, or an instant replay highlight with the area of hit circled. The agentic system can output meta-data for such visuals (e.g., coordinates of the touch). In real-time, we could overlay a colored light on the target that was hit at the moment of touch, synchronized with the commentator saying “hits the arm”. Furthermore, the timing of commentary can be improved by aligning with *natural pauses* in the sport. In fencing, after a touch, there is a short break while the referee resets the fencers. The system can use this downtime to finish the commentary before the next action begins, thus staying in sync. We could also synchronize to other audio cues: for example, ensure the commentary doesn’t clash with the referee’s announcement (if that audio is present). Advanced audio processing might mix the AI voice with environment sound such that it sounds like it’s coming through a stadium speaker or a TV broadcast (adding a bit of reverb or crowd noise ducking – purely aesthetic enhancements). All these ensure the AI’s output is *well-integrated* into the overall viewing experience, not just an isolated AI voice. If implementing offline, we can edit and polish this with precision; if live, we rely on carefully timed triggers and perhaps a slight delay on the “live” feed as noted.

In conclusion, the proposed architecture provides a **unified, end-to-end system** for automated sports commentary in fencing that works in both real-time and offline settings. It combines cutting-edge **visual AI** (VideoMAE, SAM) for deep video understanding with a **knowledge graph** for context and a **multilingual LLM** for natural language generation. The agentic workflow ensures each component operates autonomously yet collaboratively – akin to a production team – to produce commentary that is not only accurate in describing the action, but enriched with tactical insights and delivered in an engaging, human-like manner across languages. Early experiments in related domains (e.g. basketball) have shown the viability of such approaches, where key events can be recognized and commentary generated via GPT models and TTS with high accuracy ([Integrated AI System for Real-Time Sports Broadcasting: Player Behavior, Game Event Recognition, and Generative AI Commentary in Basketball Games](https://www.mdpi.com/2076-3417/15/3/1543/pdf?version=1738592773#:~:text=actions%20such%20as%20running%2C%20dribbling%2C,This%20system%20offers%20a)). Our system extends these ideas with a focus on fencing’s unique requirements and a generalizable multilingual setup. By leveraging a **unified architecture with agentic orchestration**, engineers can build upon this design to support more sports and languages, ultimately bringing fans a next-generation viewing experience where AI commentators augment the excitement of live sports with deep analytics and personalization. 

**Sources:**

- Feichtenhofer et al., *Masked Autoencoders As Spatiotemporal Learners* (VideoMAE) ([[2205.09113] Masked Autoencoders As Spatiotemporal Learners](https://arxiv.org/abs/2205.09113#:~:text=randomly%20mask%20out%20spacetime%20patches,datasets%20using%20vanilla%20Vision%20Transformers))  
- Tong et al., *VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training* ([VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training | OpenReview](https://openreview.net/forum?id=AhccnBXSne#:~:text=more%20effective%20video%20representations%20during,training%20and))  
- Kirillov et al., *Segment Anything* (SAM) ([[2304.02643] Segment Anything](https://arxiv.org/abs/2304.02643#:~:text=,1B%29%20of%201B%20masks%20and)); Yang et al., *SAMURAI: Adapting SAM for Tracking* ([[2411.11922] SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory](https://arxiv.org/abs/2411.11922#:~:text=introduces%20SAMURAI%2C%20an%20enhanced%20adaptation,AO))  
- Üstün et al., *Aya: Massively Multilingual Instruction-Following Language Model* ([[2402.07827] Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model](https://arxiv.org/abs/2402.07827#:~:text=,evaluation%2C%20and%20simulated%20win%20rates))  
- Ciric, *Knowledge Graph: The Best Friend of Generative AI* (on KGs + RAG) ([Knowledge Graph: The Best Friend of Generative AI | by Bojan Ciric | The Future of Data | Medium](https://medium.com/the-future-of-data/knowledge-graph-the-best-friend-of-generative-ai-f9e6e0f42df0#:~:text=Enhancing%20Generative%20AI%20with%20RAG,of%20reliable%20and%20rich%20knowledge))  
- Jung et al., *AI System for Real-Time Sports Broadcasting (Basketball)* – uses YOLO, R(2+1)D, GPT-4, TTS for automated commentary ([Integrated AI System for Real-Time Sports Broadcasting: Player Behavior, Game Event Recognition, and Generative AI Commentary in Basketball Games](https://www.mdpi.com/2076-3417/15/3/1543/pdf?version=1738592773#:~:text=actions%20such%20as%20running%2C%20dribbling%2C,This%20system%20offers%20a))  
- NVIDIA AI-Q (AgentIQ) – multi-agent multimodal integration (visual, speech, translation) ([NVIDIA Unveils AI-Q Blueprint to Connect AI Agents for the Future of Work | NVIDIA Blog](https://blogs.nvidia.com/blog/ai-agents-blueprint/#:~:text=AI,data%20analytics%20for%20enhanced%20intelligence))  
- Ndaliro, *Mastering Multimodal Agentic Frameworks* (agents leveraging text, images, audio) ([Mastering Function and Tool Calling for Advanced Reasoning and Multimodal Frameworks | by Emmanuel Mark Ndaliro | Medium](https://medium.com/@kram254/mastering-function-and-tool-calling-for-advanced-reasoning-and-multimodal-frameworks-fcfe6ffb6aaf#:~:text=Multimodal%20Agentic%20Frameworks))  
- Girdhar & Grauman, *Anticipative Video Transformer* (future action prediction) ([[2106.02036] Anticipative Video Transformer](https://arxiv.org/abs/2106.02036#:~:text=,AVT%20obtains%20the%20best%20reported))  
- NCSoft PAIGE – example of TTS for emotional sports broadcasting